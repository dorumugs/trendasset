#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RISE ETF êµ¬ì„±ë‚´ì—­ í¬ë¡¤ëŸ¬ (Prefect íŒŒì´í”„ë¼ì¸ ëŒ€ì‘ ë²„ì „)
------------------------------------------------
- ETF Finder í˜ì´ì§€ì—ì„œ ëª©ë¡ ë° ê° ETF ë³´ìœ  ì¢…ëª©(tab3) í¬ë¡¤ë§
- êµ¬ì„±ë‚´ì—­ JSON â†’ flatten CSV ë³€í™˜
- .env ê¸°ë°˜ KEEP_TEMP ì„¤ì • ì§€ì› (ì¤‘ê°„íŒŒì¼ ìë™ì‚­ì œ)
- ê²½ë¡œ êµ¬ì¡°: project-root/out/riseETF/, project-root/logs/
"""

import os, csv, json, time, logging, sys
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import urljoin
from datetime import datetime
from pathlib import Path
from tqdm import tqdm
from dotenv import load_dotenv
import requests
from bs4 import BeautifulSoup
import urllib3

# =====================================================
# ê²½ë¡œ ì„¤ì • (Prefect í™˜ê²½ í˜¸í™˜)
# =====================================================
BASE_DIR = Path(__file__).resolve().parents[2]
OUT_DIR = BASE_DIR / "out" / "riseETF"
LOG_DIR = BASE_DIR / "logs"

OUT_DIR.mkdir(parents=True, exist_ok=True)
LOG_DIR.mkdir(parents=True, exist_ok=True)

# =====================================================
# ë¡œê¹… ì„¤ì •
# =====================================================
log_path = LOG_DIR / f"riseetf_{time.strftime('%Y%m%d')}.log"
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_path, encoding="utf-8"),
        logging.StreamHandler(sys.stdout),
    ]
)
log = logging.getLogger(__name__)

# =====================================================
# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ (.env)
# =====================================================
load_dotenv()
KEEP_TEMP = os.getenv("KEEP_TEMP", "false").lower() in ("1", "true", "yes")

# =====================================================
# ê¸°ë³¸ ìƒìˆ˜ ì„¤ì •
# =====================================================
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

BASE = "https://riseetf.co.kr"
URL = f"{BASE}/prod/finder"
HEADERS = {
    "accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
    "accept-encoding": "gzip, deflate, br, zstd",
    "accept-language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
    "connection": "keep-alive",
    "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36"
}

# =====================================================
# â‘  ETF ê¸°ë³¸ ëª©ë¡ ìˆ˜ì§‘
# =====================================================
def scrape_rise_finder() -> Path:
    log.info("[*] ETF Finder í˜ì´ì§€ ìˆ˜ì§‘ ì¤‘ ...")
    session = requests.Session()
    try:
        r = session.get(URL, headers=HEADERS, timeout=20, verify=False)
        r.raise_for_status()
    except Exception as e:
        log.exception(f"âŒ RISE ETF í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨: {e}")
        sys.exit(1)

    soup = BeautifulSoup(r.text, "html.parser")
    rows = []
    for tr in soup.select("table tbody tr"):
        th = tr.select_one("th")
        if not th:
            continue
        name = th.get_text(strip=True)
        onclick = th.get("onclick", "")
        detail_path = onclick.split("'")[1] if "'" in onclick else ""
        detail_url = urljoin(BASE, detail_path)

        tds = tr.select("td")
        if len(tds) >= 2:
            price = tds[0].get_text(strip=True)
            change_tag = tds[1]
            direction = change_tag.select_one("span.blind")
            direction_text = direction.get_text(strip=True) if direction else ""
            change_val = change_tag.get_text(strip=True).replace(direction_text, "")
            change = f"{direction_text} {change_val}".strip()
        else:
            price = change = ""

        rows.append({
            "name": name,
            "price": price,
            "change": change,
            "detail_url": detail_url
        })

    today = datetime.now().strftime("%Y%m%d")
    out_csv = OUT_DIR / f"rise_finder_{today}.csv"

    with open(out_csv, "w", newline="", encoding="utf-8-sig") as f:
        writer = csv.DictWriter(f, fieldnames=rows[0].keys())
        writer.writeheader()
        writer.writerows(rows)

    log.info(f"âœ… {len(rows)}ê°œ ì¢…ëª© ì €ì¥ ì™„ë£Œ â†’ {out_csv}")
    return out_csv

# =====================================================
# â‘¡ ETF êµ¬ì„±ë‚´ì—­(tab3) ìˆ˜ì§‘
# =====================================================
def fetch_holdings(detail_url: str):
    """ìƒì„¸ í˜ì´ì§€ì˜ tab3 êµ¬ì„±ë‚´ì—­ì„ ë¦¬ìŠ¤íŠ¸[dict]ë¡œ ë°˜í™˜"""
    url = detail_url if "?" in detail_url else detail_url + "?searchFlag=viewtab3"
    try:
        r = requests.get(url, headers=HEADERS, timeout=15, verify=False)
        r.raise_for_status()
    except Exception as e:
        log.warning(f"âš ï¸ ìš”ì²­ ì‹¤íŒ¨: {url} ({e})")
        return []

    soup = BeautifulSoup(r.text, "html.parser")
    tbody = soup.select_one('tbody[data-class="tab3PdfList"]')
    if not tbody:
        return []

    holdings = []
    for tr in tbody.select("tr"):
        th = tr.select_one("th")
        tds = tr.select("td")
        if len(tds) == 5:
            holdings.append({
                "ë²ˆí˜¸": th.get_text(strip=True) if th else "",
                "ì¢…ëª©ëª…": tds[0].get_text(strip=True),
                "ì¢…ëª©ì½”ë“œ": tds[1].get_text(strip=True),
                "ê¸°ì¤€ê°€": tds[2].get_text(strip=True),
                "ë¹„ì¤‘(%)": tds[3].get_text(strip=True),
                "í‰ê°€ì•¡": tds[4].get_text(strip=True),
            })
    return holdings

# =====================================================
# â‘¢ ThreadPoolExecutor ë³‘ë ¬ í¬ë¡¤ë§
# =====================================================
def enrich_with_holdings_threaded(csv_path: Path, max_workers: int = 10) -> Path:
    out_csv = OUT_DIR / (csv_path.stem + "_with_holdings.csv")

    with open(csv_path, newline="", encoding="utf-8-sig") as f:
        rows = list(csv.DictReader(f))

    log.info(f"[*] ETF êµ¬ì„±ë‚´ì—­ ìˆ˜ì§‘ ì‹œì‘ ({len(rows)}ê°œ ì¢…ëª©) ...")

    results = []
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_row = {executor.submit(fetch_holdings, row["detail_url"]): row for row in rows}
        for future in tqdm(as_completed(future_to_row), total=len(rows), desc="Fetching holdings"):
            row = future_to_row[future]
            try:
                holdings = future.result()
                row["holdings"] = json.dumps(holdings, ensure_ascii=False)
            except Exception as e:
                row["holdings"] = "[]"
                log.warning(f"âš ï¸ {row['name']} ì‹¤íŒ¨: {e}")
            results.append(row)
            time.sleep(0.1)

    fieldnames = list(results[0].keys())
    with open(out_csv, "w", newline="", encoding="utf-8-sig") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(results)

    log.info(f"ğŸ’¾ êµ¬ì„±ë‚´ì—­ ìˆ˜ì§‘ ì™„ë£Œ â†’ {out_csv}")
    return out_csv

# =====================================================
# â‘£ holdings í’€ì–´ì„œ flatten CSV ìƒì„±
# =====================================================
def flatten_holdings(input_csv: Path) -> Path:
    out_csv = OUT_DIR / (input_csv.stem + "_flattened.csv")

    with open(input_csv, newline="", encoding="utf-8-sig") as f:
        reader = csv.DictReader(f)
        rows = list(reader)

    flat_rows = []
    for row in rows:
        holdings_raw = row.get("holdings", "")
        try:
            holdings = json.loads(holdings_raw)
        except json.JSONDecodeError:
            holdings = []
        for h in holdings:
            flat_rows.append({
                "name": row["name"],
                "price": row["price"],
                "change": row["change"],
                "detail_url": row["detail_url"],
                "number": h.get("ë²ˆí˜¸", ""),
                "item_name": h.get("ì¢…ëª©ëª…", ""),
                "item_code": h.get("ì¢…ëª©ì½”ë“œ", ""),
                "base_price": h.get("ê¸°ì¤€ê°€", ""),
                "ratio": h.get("ë¹„ì¤‘(%)", ""),
                "value": h.get("í‰ê°€ì•¡", ""),
            })

    fieldnames = ["name","price","change","detail_url","number","item_name","item_code","base_price","ratio","value"]
    with open(out_csv, "w", newline="", encoding="utf-8-sig") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(flat_rows)

    log.info(f"âœ… Flattened CSV ìƒì„± ì™„ë£Œ â†’ {out_csv} ({len(flat_rows)}í–‰)")
    return out_csv

# =====================================================
# â‘¤ ë©”ì¸ ì‹¤í–‰ (KEEP_TEMP ê¸°ë°˜ ì¤‘ê°„ íŒŒì¼ ì •ë¦¬)
# =====================================================
if __name__ == "__main__":
    try:
        log.info("ğŸš€ RISE ETF í¬ë¡¤ë§ ì‹œì‘")

        csv_path = scrape_rise_finder()                         # â‘  ê¸°ë³¸ ETF ë¦¬ìŠ¤íŠ¸
        enriched_csv = enrich_with_holdings_threaded(csv_path)   # â‘¡ holdings ì¶”ê°€
        final_csv = flatten_holdings(enriched_csv)               # â‘¢ flatten ìµœì¢…ë³¸ ìƒì„±

        if KEEP_TEMP:
            log.info("ğŸ—‚ ì¤‘ê°„ íŒŒì¼ ë³´ì¡´ (.env KEEP_TEMP=true)")
        else:
            for fp in [csv_path, enriched_csv]:
                try:
                    if Path(fp).exists():
                        Path(fp).unlink()
                        log.info(f"ğŸ§¹ ì¤‘ê°„ íŒŒì¼ ì‚­ì œ ì™„ë£Œ: {Path(fp).name}")
                except Exception as e:
                    log.warning(f"[WARN] ì¤‘ê°„ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {fp} ({e})")

        log.info(f"âœ… RISE ETF íŒŒì´í”„ë¼ì¸ ì™„ë£Œ â†’ {Path(final_csv).name}")

    except Exception as e:
        log.exception(f"âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)
