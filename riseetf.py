#!/usr/bin/env python3
import csv, json, time
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import urljoin
from datetime import datetime
from pathlib import Path
from tqdm import tqdm
import requests
from bs4 import BeautifulSoup

import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

BASE = "https://riseetf.co.kr"
URL = f"{BASE}/prod/finder"

HEADERS = {
    "accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
    "accept-encoding": "gzip, deflate, br, zstd",
    "accept-language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
    "connection": "keep-alive",
    "cookie": "_ga=GA1.1.1323944014.1760681288; ETF_SESSIONID1=6396318a-935e-4814-a10c-517956f719f8; JSESSIONID=Jf6mpXTainfji2kJo1vVcOul7mCALRDerPYTa41CD1oo8XP1tnRtNBkfmnSv0P1t.amV1c19kb21haW4vZXRm; _ga_83VQBEQXZ2=GS2.1.s1762493466$o9$g1$t1762494131$j60$l0$h0",
    "host": "riseetf.co.kr",
    "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36"
}

# -----------------------------
# â‘  ETF ê¸°ë³¸ ëª©ë¡ ìˆ˜ì§‘
# -----------------------------
def scrape_rise_finder():
    session = requests.Session()
    r = session.get(URL, headers=HEADERS, timeout=20, verify=False)
    r.raise_for_status()
    soup = BeautifulSoup(r.text, "html.parser")

    rows = []
    for tr in soup.select("table tbody tr"):
        th = tr.select_one("th")
        if not th:
            continue
        name = th.get_text(strip=True)
        onclick = th.get("onclick", "")
        detail_path = onclick.split("'")[1] if "'" in onclick else ""
        detail_url = urljoin(BASE, detail_path)

        tds = tr.select("td")
        if len(tds) >= 2:
            price = tds[0].get_text(strip=True)
            change_tag = tds[1]
            direction = change_tag.select_one("span.blind")
            direction_text = direction.get_text(strip=True) if direction else ""
            change_val = change_tag.get_text(strip=True).replace(direction_text, "")
            change = f"{direction_text} {change_val}".strip()
        else:
            price = change = ""

        rows.append({
            "name": name,
            "price": price,
            "change": change,
            "detail_url": detail_url
        })

    today = datetime.now().strftime("%Y%m%d")
    out_dir = Path("out/riseETF")
    out_dir.mkdir(parents=True, exist_ok=True)
    out_csv = out_dir / f"rise_finder_{today}.csv"

    with open(out_csv, "w", newline="", encoding="utf-8-sig") as f:
        writer = csv.DictWriter(f, fieldnames=rows[0].keys())
        writer.writeheader()
        writer.writerows(rows)

    print(f"âœ… {len(rows)}ê°œ ì¢…ëª© ì €ì¥ ì™„ë£Œ â†’ {out_csv}")
    return out_csv


# -----------------------------
# â‘¡ ETF êµ¬ì„± ë‚´ì—­ ìˆ˜ì§‘ (tab3)
# -----------------------------
def fetch_holdings(detail_url: str):
    """ìƒì„¸ í˜ì´ì§€ì˜ tab3 êµ¬ì„±ë‚´ì—­ì„ ë¦¬ìŠ¤íŠ¸[dict]ë¡œ ë°˜í™˜"""
    url = detail_url if "?" in detail_url else detail_url + "?searchFlag=viewtab3"
    try:
        r = requests.get(url, headers=HEADERS, timeout=15, verify=False)
        r.raise_for_status()
    except Exception as e:
        return []

    soup = BeautifulSoup(r.text, "html.parser")
    tbody = soup.select_one('tbody[data-class="tab3PdfList"]')
    if not tbody:
        return []

    holdings = []
    for tr in tbody.select("tr"):
        th = tr.select_one("th")
        tds = tr.select("td")
        if len(tds) == 5:
            holdings.append({
                "ë²ˆí˜¸": th.get_text(strip=True) if th else "",
                "ì¢…ëª©ëª…": tds[0].get_text(strip=True),
                "ì¢…ëª©ì½”ë“œ": tds[1].get_text(strip=True),
                "ê¸°ì¤€ê°€": tds[2].get_text(strip=True),
                "ë¹„ì¤‘(%)": tds[3].get_text(strip=True),
                "í‰ê°€ì•¡": tds[4].get_text(strip=True),
            })
    return holdings


# -----------------------------
# â‘¢ ThreadPoolExecutor ë³‘ë ¬ í¬ë¡¤ë§
# -----------------------------
def enrich_with_holdings_threaded(csv_path: Path, max_workers: int = 15):
    out_dir = csv_path.parent
    out_csv = out_dir / (csv_path.stem + "_with_holdings.csv")

    # ì›ë³¸ CSV ì½ê¸°
    with open(csv_path, newline="", encoding="utf-8-sig") as f:
        rows = list(csv.DictReader(f))

    # ë³‘ë ¬ ì‹¤í–‰
    results = []
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_row = {executor.submit(fetch_holdings, row["detail_url"]): row for row in rows}
        for future in tqdm(as_completed(future_to_row), total=len(rows), desc="Fetching holdings"):
            row = future_to_row[future]
            try:
                holdings = future.result()
                row["holdings"] = json.dumps(holdings, ensure_ascii=False)
            except Exception as e:
                row["holdings"] = "[]"
                print(f"âš ï¸ {row['name']} ì‹¤íŒ¨: {e}")
            results.append(row)

    # CSV ì €ì¥
    fieldnames = list(results[0].keys())
    with open(out_csv, "w", newline="", encoding="utf-8-sig") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(results)

    print(f"\nğŸ’¾ ì™„ì„± â†’ {out_csv}")
    return out_csv


# -----------------------------
# â‘£ holdings í’€ì–´ì„œ flatten CSV ìƒì„±
# -----------------------------
def flatten_holdings(input_csv: Path):
    out_csv = input_csv.with_name(input_csv.stem + "_flattened.csv")

    with open(input_csv, newline="", encoding="utf-8-sig") as f:
        reader = csv.DictReader(f)
        rows = list(reader)

    flat_rows = []
    for row in rows:
        holdings_raw = row.get("holdings", "")
        try:
            holdings = json.loads(holdings_raw)
        except json.JSONDecodeError:
            holdings = []

        for h in holdings:
            flat_rows.append({
                "name": row["name"],
                "price": row["price"],
                "change": row["change"],
                "detail_url": row["detail_url"],
                "number": h.get("ë²ˆí˜¸", ""),
                "item_name": h.get("ì¢…ëª©ëª…", ""),
                "item_code": h.get("ì¢…ëª©ì½”ë“œ", ""),
                "base_price": h.get("ê¸°ì¤€ê°€", ""),
                "ratio": h.get("ë¹„ì¤‘(%)", ""),
                "value": h.get("í‰ê°€ì•¡", ""),
            })

    # ì €ì¥
    fieldnames = ["name", "price", "change", "detail_url",
                  "number", "item_name", "item_code", "base_price", "ratio", "value"]

    with open(out_csv, "w", newline="", encoding="utf-8-sig") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(flat_rows)

    print(f"âœ… Flattened CSV ìƒì„± ì™„ë£Œ â†’ {out_csv}")
    print(f"ì´ {len(flat_rows)}í–‰ ë³€í™˜ë¨")
    return out_csv


# -----------------------------
# ì‹¤í–‰
# -----------------------------
if __name__ == "__main__":
    csv_path = scrape_rise_finder()                                # â‘  ê¸°ë³¸ ETF ë¦¬ìŠ¤íŠ¸
    enriched_csv = enrich_with_holdings_threaded(csv_path)          # â‘¡ holdings ì¶”ê°€
    flatten_holdings(enriched_csv)                                  # â‘¢ flatten CSV ìƒì„±
